{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27be6bc1",
      "metadata": {},
      "source": [
        "# **Variational AutoEncoder for Anomaly Detection**\n",
        "\n",
        "## Premise\n",
        "The Variational Autoencoder (VAE) is a generative model that learns to encode data into a lower-dimensional latent space and then decode it back to reconstruct the original data. For anomaly detection, we exploit the fact that VAEs learn the normal data distribution, making anomalies result in higher reconstruction errors.\n",
        "\n",
        "## Core Concepts with Simple Example\n",
        "Let's use a simple 2D point dataset to illustrate each step.\n",
        "\n",
        "Example Dataset:\n",
        "```python\n",
        "# Normal data: Points roughly following a circular pattern\n",
        "normal_points = [\n",
        "    (1.1, 0.8), (0.9, 1.2), (-0.8, 1.1), (-1.1, -0.9),\n",
        "    (0.8, -1.1), (0.1, 0.2), (-0.2, -0.1)\n",
        "]\n",
        "# Anomaly: Point far from the pattern\n",
        "anomaly = (4.0, 4.0)\n",
        "```\n",
        "\n",
        "## Derivation of the Loss Function used in Variational AutoEncoder\n",
        "\n",
        "### 1. Bayes' Theorem:\n",
        "$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$\n",
        "\n",
        "*Example Interpretation:*\n",
        "- x: Our 2D point (1.1, 0.8)\n",
        "- z: Lower dimensional latent representation (e.g., single number)\n",
        "- p(z|x): Probability of latent code given our point\n",
        "- p(x|z): Probability of reconstructing our point from the latent code\n",
        "- p(z): Prior beliefs about latent codes (standard normal distribution)\n",
        "- p(x): Total probability of observing the point\n",
        "\n",
        "### 2. Log of Marginal Likelihood:\n",
        "$\\log p(x) = \\log \\int p(x|z)p(z) \\, dz$\n",
        "\n",
        "*Example:*\n",
        "For our point (1.1, 0.8), we'd need to:\n",
        "1. Consider all possible latent codes z\n",
        "2. For each z, multiply:\n",
        "   - Probability of reconstructing (1.1, 0.8) from z\n",
        "   - Probability of that z occurring\n",
        "3. Sum all these products (integral)\n",
        "4. Take the log\n",
        "\n",
        "### 3. Variational Approximation ($q(z|x)$):\n",
        "Introduce a simpler distribution $q(z|x)$ to approximate the posterior $p(z|x)$:\n",
        "$\\log p(x) = \\log \\int p(x|z)p(z) \\frac{q(z|x)}{q(z|x)} \\, dz$\n",
        "\n",
        "*Example:*\n",
        "Instead of computing exact probabilities, we use a neural network (encoder) to predict:\n",
        "- Mean (μ) and variance (σ²) of a Gaussian distribution for our point (1.1, 0.8)\n",
        "- e.g., μ = 0.5, σ² = 0.1\n",
        "\n",
        "### 4. Jensen's Inequality (Lower Bound):\n",
        "$$\\begin{aligned}\n",
        "\\log p(x) &= \\log \\mathbb{E}_{q(z|x)}\\left[\\frac{p(x|z)p(z)}{q(z|x)}\\right] \\\\\n",
        "&\\geq \\int q(z|x) \\log \\frac{p(x|z)p(z)}{q(z|x)} \\, dz \\\\\n",
        "&= \\mathbb{E}_{q(z|x)} [ \\log p(x|z) + \\log p(z) - \\log q(z|x) ]\n",
        "\\end{aligned}$$\n",
        "\n",
        "*Example:*\n",
        "For our point, this means:\n",
        "1. Sample latent codes from our predicted distribution (μ = 0.5, σ² = 0.1)\n",
        "2. For each sample:\n",
        "   - Reconstruct the point\n",
        "   - Calculate reconstruction probability\n",
        "   - Add prior probability\n",
        "   - Subtract encoding probability\n",
        "3. Average these values\n",
        "\n",
        "### 5. Simplification with Gaussian Assumptions:\n",
        "- **Prior**: $ p(z) = \\mathcal{N}(z; 0, I) $\n",
        "- **Variational Posterior**: $ q(z|x) = \\mathcal{N}(z; \\mu_q(x), \\text{diag}(\\sigma_q^2(x))) $\n",
        "- **Likelihood**: $ p(x|z) = \\mathcal{N}(x; f(z), \\sigma^2I) $\n",
        "\n",
        "*Example:*\n",
        "For (1.1, 0.8):\n",
        "- Prior: Assume latent codes follow standard normal\n",
        "- Encoder predicts: μ = 0.5, σ² = 0.1\n",
        "- Decoder predicts: reconstruction = (1.0, 0.9)\n",
        "\n",
        "### 6. Reconstruction Error (Gaussian Likelihood):\n",
        "$$\\mathbb{E}_{q(z|x)} [ \\log p(x|z) ] = \\mathbb{E}_{q(z|x)}\\left[-\\frac{1}{2\\sigma^2}\\|x - f(z)\\|^2 - \\frac{n}{2}\\log(2\\pi\\sigma^2)\\right]$$\n",
        "\n",
        "*Example:*\n",
        "- Original point: (1.1, 0.8)\n",
        "- Reconstructed point: (1.0, 0.9)\n",
        "- Error = √((1.1-1.0)² + (0.8-0.9)²) = 0.141\n",
        "\n",
        "### 7. KL Divergence:\n",
        "$$D_{KL}(q(z|x) \\| p(z)) = \\frac{1}{2}\\sum_{i=1}^{d} (\\sigma_{q,i}^2 + \\mu_{q,i}^2 - \\log \\sigma_{q,i}^2 - 1)$$\n",
        "\n",
        "*Example:*\n",
        "For our predicted distribution (μ = 0.5, σ² = 0.1):\n",
        "KL = 0.5 * (0.1 + 0.5² - log(0.1) - 1) = 1.35\n",
        "\n",
        "### 8. Reparameterization Trick:\n",
        "$$z = \\mu_q(x) + \\sigma_q(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "*Example:*\n",
        "Instead of sampling directly from N(0.5, 0.1):\n",
        "1. Sample ε ~ N(0, 1), e.g., ε = 0.3\n",
        "2. Compute: z = 0.5 + √0.1 * 0.3 = 0.595\n",
        "\n",
        "### 9. Complete ELBO Loss:\n",
        "$$\\begin{aligned}\n",
        "\\text{ELBO} &= \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[-\\frac{1}{2\\sigma^2}\\|x - f(\\mu_q(x) + \\sigma_q(x) \\odot \\epsilon)\\|^2\\right] \\\\\n",
        "&- \\frac{1}{2}\\sum_{i=1}^{d} (\\sigma_{q,i}^2 + \\mu_{q,i}^2 - \\log \\sigma_{q,i}^2 - 1) - \\frac{n}{2}\\log(2\\pi\\sigma^2)\n",
        "\\end{aligned}$$\n",
        "\n",
        "*Example:*\n",
        "Combining:\n",
        "- Reconstruction error: 0.141\n",
        "- KL divergence: 1.35\n",
        "ELBO ≈ -0.141 - 1.35 = -1.491\n",
        "\n",
        "### 10. Final Simplified ELBO Loss with $\\beta$ (β-VAE):\n",
        "$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[-\\frac{1}{2\\sigma^2}\\|x - f(\\mu_q(x) + \\sigma_q(x) \\odot \\epsilon)\\|^2\\right] - \\beta D_{KL}(q(z|x)\\|p(z))$$\n",
        "\n",
        "*Example with β = 0.5:*\n",
        "$$\\mathcal{L}_{\\beta\\text{-VAE}} = -0.141 - 0.5 * 1.35 = -0.816$$\n",
        "\n",
        "For anomaly detection:\n",
        "- Normal point (1.1, 0.8): Loss = -0.816\n",
        "- Anomaly point (4.0, 4.0): Loss ≈ -5.234 (much higher reconstruction error)\n",
        "\n",
        "This higher loss for the anomaly point indicates it doesn't fit the learned normal data distribution, allowing us to detect it as an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e6b920d2-3954-49ae-be02-4e51082e463b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install -q pyod==2.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e591f31f-d304-428a-94d1-b1c78e766139",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████████████████████████████| 30/30 [00:00<00:00, 31.24it/s]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pyod.models.vae import VAE\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from pyod.utils.data import (\n",
        "    generate_data, evaluate_print\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score, f1_score\n",
        ")\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "# Generate synthetic data\n",
        "contamination = 0.1\n",
        "n_train = 1000\n",
        "n_test = 100\n",
        "n_features = 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = generate_data(\n",
        "    n_train=n_train, n_test=n_test, \n",
        "    n_features=n_features,\n",
        "    contamination=contamination, random_state=1\n",
        ")\n",
        "\n",
        "# Train the VAE model\n",
        "clf_name_vae = 'VAE'\n",
        "vae_clf = VAE(epoch_num=30, \n",
        "              contamination=contamination, \n",
        "              beta=1.0)\n",
        "vae_clf.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "096ce6fd-665c-401b-bf16-aa280bbdd2ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████████████████████████████| 30/30 [00:00<00:00, 44.15it/s]\n"
          ]
        }
      ],
      "source": [
        "# Train the AE model\n",
        "clf_name_ae = 'AE'\n",
        "ae_clf = AutoEncoder(epoch_num=30, \n",
        "                     contamination=contamination)\n",
        "ae_clf.fit(X_train)\n",
        "\n",
        "# Predictions and scores for VAE\n",
        "y_test_pred_vae = vae_clf.predict(X_test)\n",
        "\n",
        "# Predictions and scores for AE\n",
        "y_test_pred_ae = ae_clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score, f1_score\n",
        ")\n",
        "\n",
        "# Compute metrics function\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    balanced_acc = balanced_accuracy_score(\n",
        "        y_true, y_pred\n",
        "    )\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return balanced_acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556a185a-a49d-4c56-9654-cbf978a06531",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_detailed_results(X, y_true, y_pred, model_name, dataset_name, ax):\n",
        "    # Compute metrics\n",
        "    balanced_acc, f1 = compute_metrics(y_true, y_pred)\n",
        "    \n",
        "    # Plot points with different categories\n",
        "    ax.scatter(X[(y_true == 1) & (y_pred == 1), 0], X[(y_true == 1) & (y_pred == 1), 1], \n",
        "               c='red', marker='x', label='True Positive (Anomaly)')\n",
        "    ax.scatter(X[(y_true == 0) & (y_pred == 0), 0], X[(y_true == 0) & (y_pred == 0), 1], \n",
        "               c='green', marker='+', label='True Negative (Non-Anomaly)')\n",
        "    ax.scatter(X[(y_true == 0) & (y_pred == 1), 0], X[(y_true == 0) & (y_pred == 1), 1], \n",
        "               c='orange', marker='*', label='False Positive (Non-Anomaly)')\n",
        "    ax.scatter(X[(y_true == 1) & (y_pred == 0), 0], X[(y_true == 1) & (y_pred == 0), 1], \n",
        "               c='blue', marker='^', label='False Negative (Anomaly)')\n",
        "    \n",
        "    # Title with metrics\n",
        "    ax.set_title(f\"{model_name} - {dataset_name}\\nBalanced Acc: {balanced_acc:.2f}, F1: {f1:.2f}\")\n",
        "    ax.set_xlabel(\"Feature 1\")\n",
        "    ax.set_ylabel(\"Feature 2\")\n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(9, 9), dpi=300)\n",
        "\n",
        "# Visualize results for VAE on test data\n",
        "visualize_detailed_results(X_test, y_test, y_test_pred_vae, \"VAE\", \"Test Data\", axes[0, 0])\n",
        "\n",
        "# Visualize results for AE on test data\n",
        "visualize_detailed_results(X_test, y_test, y_test_pred_ae, \"AE\", \"Test Data\", axes[0, 1])\n",
        "\n",
        "# Visualize results for VAE on training data\n",
        "visualize_detailed_results(X_train, y_train, vae_clf.labels_, \"VAE\", \"Training Data\", axes[1, 0])\n",
        "\n",
        "# Visualize results for AE on training data\n",
        "visualize_detailed_results(X_train, y_train, ae_clf.labels_, \"AE\", \"Training Data\", axes[1, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb44f10f-02bf-4bf0-b73d-cccd4813a835",
      "metadata": {},
      "source": [
        "| **Aspect**                   | **Autoencoder (AE)**                                      | **Variational Autoencoder (VAE)**                          |\n",
        "|------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
        "| **Latent Space**              | Deterministic representation (fixed point for each input) | Probabilistic representation (distribution over latent variables) |\n",
        "| **Latent Space Structure**    | No specific regularization; can be scattered and unstructured | Regularized to match a predefined prior (typically Gaussian), resulting in a smooth, continuous space |\n",
        "| **Objective**                 | Minimize reconstruction error (e.g., MSE)                 | Minimize both reconstruction error and KL divergence to enforce latent space structure |\n",
        "| **Encoder Output**            | Direct mapping to a single point in latent space          | Outputs parameters of a distribution (mean and variance) for each latent variable |\n",
        "| **Generative Capability**     | Limited generative ability; may not generalize well for new data | Strong generative capability due to regularized latent space |\n",
        "| **Latent Variable Interpolation** | Less smooth interpolation between latent variables        | Smooth interpolation due to the continuous nature of the latent space |\n",
        "| **KL Divergence**             | Not used in the loss function                             | KL divergence term in the loss function regularizes the latent space |\n",
        "| **Reconstruction**            | Reconstructs the input deterministically                  | Reconstructs the input probabilistically, sampling from the learned latent distribution |\n",
        "| **Use Cases**                 | Mainly used for dimensionality reduction and reconstruction tasks | Used for generative modeling, data generation, and anomaly detection |\n",
        "| **Regularization**            | None                                                     | Explicit regularization to ensure the latent space follows a known distribution (e.g., Gaussian) |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "book",
      "language": "python",
      "name": "book"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
