{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoding Strategies: Greedy vs Beam\n",
        "\n",
        "**Reference: https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "np.random.seed(47)\n",
        "torch.manual_seed(47)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_log_probability(logits, token_id):\n",
        "    \"\"\"Calculate log probability for a token.\"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = torch.log(probs)\n",
        "    return log_probs[token_id].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_search_tree(graph, num_tokens_to_generate, beam_width, title=\"Search Tree\"):\n",
        "    \"\"\"Visualize search results as a tree.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(3 + 1.15 * beam_width**num_tokens_to_generate, \n",
        "                                   max(5, 2 + num_tokens_to_generate)), \n",
        "                          dpi=300)\n",
        "    \n",
        "    pos = nx.nx_agraph.graphviz_layout(graph, prog='dot')\n",
        "    scores = [data['sequencescore'] for _, data in graph.nodes(data=True)]\n",
        "    \n",
        "    norm = Normalize(vmin=min(scores), vmax=max(scores))\n",
        "    cmap = LinearSegmentedColormap.from_list('rg', ['r', 'y', 'g'], N=5)\n",
        "    \n",
        "    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_color=scores, cmap=cmap)\n",
        "    nx.draw_networkx_edges(graph, pos)\n",
        "    \n",
        "    labels = {node: f\"{data['token'].split('_')[0]}\\n{data['sequencescore']:.2f}\"\n",
        "             for node, data in graph.nodes(data=True)}\n",
        "    \n",
        "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n",
        "    plt.title(title)\n",
        "    plt.box(False)\n",
        "    \n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    fig.colorbar(sm, ax=ax, pad=0, label='Sequence Score')\n",
        "\n",
        "    fig.savefig(f'{title}.png', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_search(input_ids, start_node, num_tokens_to_generate=1):\n",
        "    \"\"\"Implement greedy search.\"\"\"\n",
        "    current_node = start_node\n",
        "    cumulative_score = 0\n",
        "    \n",
        "    for i in range(num_tokens_to_generate):\n",
        "        logits = model(input_ids).logits[0, -1, :]\n",
        "        next_token_id = torch.argmax(logits)\n",
        "        score = get_log_probability(logits, next_token_id)\n",
        "        next_token = tokenizer.decode(next_token_id, skip_special_tokens=True)\n",
        "        \n",
        "        input_ids = torch.hstack([input_ids, next_token_id[None, None]])\n",
        "        current_node = list(graph.successors(current_node))[0]\n",
        "        cumulative_score += score\n",
        "        \n",
        "        graph.nodes[current_node].update({\n",
        "            'sequencescore': cumulative_score / len(input_ids.squeeze()),\n",
        "            'token': next_token + f'_{num_tokens_to_generate - i}'\n",
        "        })\n",
        "        \n",
        "    return input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search(input_ids, start_node, num_tokens_to_generate, beam_width):\n",
        "    \"\"\"Implement beam search.\"\"\"\n",
        "    stack = [(input_ids, start_node, num_tokens_to_generate, 0)]\n",
        "    \n",
        "    while stack:\n",
        "        current_input_ids, node, tokens_remaining, cumulative_score = stack.pop()\n",
        "        \n",
        "        if tokens_remaining == 0:\n",
        "            continue\n",
        "            \n",
        "        logits = model(current_input_ids).logits[0, -1, :]\n",
        "        top_token_ids = torch.topk(logits, beam_width).indices\n",
        "\n",
        "        for i in range(len(top_token_ids)-1, -1, -1):\n",
        "            next_token_id = top_token_ids[i]\n",
        "            score = get_log_probability(logits, next_token_id)\n",
        "            new_score = cumulative_score + score\n",
        "            \n",
        "            new_input_ids = torch.hstack([current_input_ids, next_token_id[None, None]])\n",
        "            token = tokenizer.decode(next_token_id, skip_special_tokens=True)\n",
        "            current_node = list(graph.successors(node))[i]\n",
        "            \n",
        "            graph.nodes[current_node].update({\n",
        "                'sequencescore': new_score / len(new_input_ids.squeeze()),\n",
        "                'token': token + f'_{tokens_remaining}_{i}'\n",
        "            })\n",
        "            \n",
        "            stack.append((new_input_ids, current_node, tokens_remaining-1, new_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sequence(G, node=None):\n",
        "    \"\"\"Get the sequence from root to given node (or best leaf).\"\"\"\n",
        "    if node is None:\n",
        "        leaf_nodes = [n for n in G.nodes() if G.out_degree(n) == 0]\n",
        "        node = max(leaf_nodes, key=lambda n: G.nodes[n]['sequencescore'])\n",
        "    \n",
        "    path = nx.shortest_path(G, source=0, target=node)\n",
        "    sequence = ''.join([G.nodes[n]['token'].split('_')[0] for n in path])\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Never give\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "num_tokens_to_generate = 2\n",
        "beam_width = 2\n",
        "\n",
        "# Run Greedy Search\n",
        "graph_greedy = nx.balanced_tree(1, num_tokens_to_generate, create_using=nx.DiGraph())\n",
        "for node in graph_greedy.nodes:\n",
        "    graph_greedy.nodes[node].update({\n",
        "        'sequencescore': 0,\n",
        "        'token': text\n",
        "    })\n",
        "\n",
        "graph = graph_greedy\n",
        "greedy_output = greedy_search(input_ids, 0, num_tokens_to_generate)\n",
        "plot_search_tree(graph_greedy, num_tokens_to_generate, 1, \"Greedy Search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Run Beam Search\n",
        "graph_beam = nx.balanced_tree(beam_width, num_tokens_to_generate, create_using=nx.DiGraph())\n",
        "for node in graph_beam.nodes:\n",
        "    graph_beam.nodes[node].update({\n",
        "        'sequencescore': 0,\n",
        "        'token': text\n",
        "    })\n",
        "\n",
        "graph = graph_beam\n",
        "beam_search(input_ids, 0, num_tokens_to_generate, beam_width)\n",
        "plot_search_tree(graph_beam, num_tokens_to_generate, beam_width, \"Beam Search\")\n",
        "\n",
        "print(\"Greedy search output:\", get_sequence(graph_greedy))\n",
        "print(\"Beam search output:\", get_sequence(graph_beam))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "book",
      "language": "python",
      "name": "book"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
