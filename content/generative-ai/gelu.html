
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Intuition Behind GeLU &#8212; Data and AI Concepts</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/generative-ai/gelu';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Orthogonal Matrix" href="orthogonality.html" />
    <link rel="prev" title="Attention Mechanism" href="attention_mechanism.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Data and AI Concepts - Home"/>
    <img src="../../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Data and AI Concepts - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative AI ü§ñ</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/gaussian_mixture_models.html">Gaussian Mixture Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="large-language-modelling/causal_attention.html">Causal Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="large-language-modelling/decoding_strategies.html">Text Decoding Strategies--Greedy vs Beam</a></li>
<li class="toctree-l1"><a class="reference internal" href="large-language-modelling/layer_and_rms_normalization.html">Layer vs RMS Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="large-language-modelling/multi_head_attention.html">Multi-head Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="variational_autoencoders/autoencoder_latent_space.html">Ideal Properties of a Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="variational_autoencoders/pca_for_anomaly_detection.html">PCA for Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="variational_autoencoders/vae_anomaly_detection.html">Variational AutoEncoder for Anomaly Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="variational_autoencoders/vae_on_mnist.html">Variational AutoEncoder Loss Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention_mechanism.html">Attention Mechanism</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">GELU</a></li>
<li class="toctree-l1"><a class="reference internal" href="orthogonality.html">Orthogonality</a></li>
<li class="toctree-l1"><a class="reference internal" href="perplexity.html">Perplexity</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning üß†</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/focal_loss_balanced.html">Balanced Focal Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/jensen_inequality.html">Jensen's Inequality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/reparametrization_trick.html">Reparametrization Trick</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/temperature_scaled_softmax.html">Temperature Scaled Softmax</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Interpretable AI üîç</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../interpretable-ai/logistic_regression.html">Logistic Regression Coefficient Interpretation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interpretable-ai/shapley.html">Shapley values and SHAP for ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../interpretable-ai/model_counterfactuals.html">Counterfactuals</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning üîß</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/trees-and-forests/gini_impurity_vs_entropy.html">Gini Impurity vs Entropy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/agglomerative_clustering.html">Agglomerative Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/elastic_net.html">Elastic Net</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/huber_loss.html">Huber Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/mahalanobis_distance.html">Mahalanobis Distance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/natural_breaks.html">Natural Breaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/oversampling.html">Oversampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/pca_vs_feat_ag.html">PCA vs Feature Agglomeration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/permutation_importance.html">Permutation Importance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machine-learning/pseudo_r2.html">Pseudo R¬≤</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mathematical Statistics üé≤</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/chebyshev_inequality.html">Chebyshev's Inequality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/dist_of_minimum.html">Distribution of Minimum</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/matrix_calculus_short.html">Matrix Calculus Jacobians and Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/multivariate_normal_distribution.html">Multivariate Normal Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/mutual_information.html">Mutual Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/point_biserial.html">Point Biserial Correlation Coefficient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/unbiased_vs_consistent.html">Unbiasesd vs Consistent Estimator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathematical-statistics/ecdf.html">ECDF</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Applied Statistics üìä</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/acf_and_pacf.html">Autocorrelation Function vs Partial Autocorrelation Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/adjusted_r_squared.html">Adjusted R¬≤</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/condition_number.html">Condition Number</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/cramer_v.html">Cramer's V</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/ewa_and_bias_correction.html">Exponentially Weighted Average and Bias Correction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/kruskal_wallis.html">Kruskal Wallis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/kendalltaub.html">Kendall's Rank Correlation Coefficient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/spurious_correlation.html">Spurious Correlation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applied-statistics/predictive_r2.html">Leave One Out Cross Validation and PRESS</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Multivariate Statistics üìà</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../multivariate-statistics/canonical_correlation_analysis.html">Canonical Correlation Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate-statistics/correspondence_analysis.html">Correspondence Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate-statistics/factor_analysis.html">Factor Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate-statistics/hotelling.html">Hotelling's T¬≤</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate-statistics/principal_component_analysis.html">Principal Component Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Prerequisite Mathematics ‚ûó</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../prerequisite-math/linear-algebra/introduction.html">The Origin</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/energy.html">Energy</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/hyperplanes.html">Hyperplanes</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/inner_product.html"><strong>Inner Product</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/moore_penrose_inverse.html">Moore Penrose Inverse</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/multiclass_classification.html">Jacobians and Gradients behind Multi-class Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/norm_and_metric.html">Norm and Metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix-calculus-for-genAI/rank_one_matrices.html"><strong>Rank-1 Matrices</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../prerequisite-math/linear-algebra/spectral_decomposition.html">Spectral Decomposition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Programming üíª</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../programming/leetcode-python/kadanes.html">Kadane's Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/leetcode-python/prefix_sum.html">Prefix Sum and Sliding Window</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/leetcode-python/two_pointer.html">Two Pointer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/pandas/pivoting.html">Pivoting in Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../programming/optimization/cudf.html">Pandas on NVIDIA GPUs</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/AIinMinutes/Data-and-AI-Concepts" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/AIinMinutes/Data-and-AI-Concepts/issues/new?title=Issue%20on%20page%20%2Fcontent/generative-ai/gelu.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/generative-ai/gelu.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Intuition Behind GeLU</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-interactions">Key Interactions:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu-vs-common-activation-functions">GeLU vs Common Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-relu">1. Problems with ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-non-differentiability-at-x-0">a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-dead-neurons">b. Dead Neurons</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-harsh-transitions">c. Harsh Transitions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-sigmoid">2. Problems with Sigmoid</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-vanishing-gradients-for-large-inputs">a. Vanishing Gradients for Large Inputs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-non-zero-mean-outputs">b. Non-zero Mean Outputs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-tanh">3. Problems with Tanh</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-vanishing-gradients-for-extreme-inputs">a. Vanishing Gradients for Extreme Inputs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-computational-complexity">b. Computational Complexity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-leaky-relu">4. Problems with Leaky ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-arbitrary-slope-for-negative-inputs">b. Arbitrary Slope for Negative Inputs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complication-with-swish">5. Complication with Swish</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-additional-learnable-parameter">a. Additional Learnable Parameter</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-problems-in-activation-functions">6. General Problems in Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-saturation">a. Saturation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-lack-of-smooth-transitions">b. Lack of Smooth Transitions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="intuition-behind-gelu">
<h1>Intuition Behind GeLU<a class="headerlink" href="#intuition-behind-gelu" title="Link to this heading">#</a></h1>
<p>In GeLU, the output is <span class="math notranslate nohighlight">\(x \Phi(x)\)</span>, where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the cumulative distribution function (CDF) of the standard normal distribution. This combines <span class="math notranslate nohighlight">\(x\)</span> with a probabilistic weighting based on its magnitude and sign, as determined by the CDF.</p>
<section id="key-interactions">
<h2>Key Interactions:<a class="headerlink" href="#key-interactions" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Large positive <span class="math notranslate nohighlight">\(x\)</span></strong>:<br />
<span class="math notranslate nohighlight">\(\Phi(x) \approx 1\)</span>, so <span class="math notranslate nohighlight">\(x\)</span> is kept almost as-is. These values are important and should be retained.</p></li>
<li><p><strong>Small positive <span class="math notranslate nohighlight">\(x\)</span></strong>:<br />
<span class="math notranslate nohighlight">\(\Phi(x) &lt; 1\)</span>, so <span class="math notranslate nohighlight">\(x\)</span> is scaled down. These values are less critical but still contribute.</p></li>
<li><p><strong>Large negative <span class="math notranslate nohighlight">\(x\)</span></strong>:<br />
<span class="math notranslate nohighlight">\(\Phi(x) \approx 0\)</span>, so <span class="math notranslate nohighlight">\(x\)</span> is suppressed to reduce large negative contributions, helping to prevent destabilizing large negative summations in the overall output.</p></li>
<li><p><strong>Near-zero <span class="math notranslate nohighlight">\(x\)</span></strong>:<br />
The function smoothly transitions, ensuring differentiability and avoiding sharp cuts like ReLU.</p></li>
</ul>
<p><strong>Note</strong>: GeLU is a deterministic function. There is no random sampling happening. The key idea is that input values are modulated according to their magnitude and sign, using the cumulative distribution function (CDF) of the standard normal distribution.</p>
</section>
<section id="gelu-vs-common-activation-functions">
<h2>GeLU vs Common Activation Functions<a class="headerlink" href="#gelu-vs-common-activation-functions" title="Link to this heading">#</a></h2>
<section id="problems-with-relu">
<h3>1. Problems with ReLU<a class="headerlink" href="#problems-with-relu" title="Link to this heading">#</a></h3>
<section id="a-non-differentiability-at-x-0">
<h4>a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span><a class="headerlink" href="#a-non-differentiability-at-x-0" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>ReLU</strong>: Sharp transition at <span class="math notranslate nohighlight">\( x = 0 \)</span>, causing non-differentiability and potential unstable gradients.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU is smooth and differentiable everywhere, allowing stable gradient flow and better convergence.</p></li>
</ul>
</section>
<section id="b-dead-neurons">
<h4>b. Dead Neurons<a class="headerlink" href="#b-dead-neurons" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>ReLU</strong>: Negative outputs are set to zero, potentially leading to neurons that never activate again if their gradients vanish.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU modulates negative values rather than zeroing them out, retaining some learning ability for negative inputs.</p></li>
</ul>
</section>
<section id="c-harsh-transitions">
<h4>c. Harsh Transitions<a class="headerlink" href="#c-harsh-transitions" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>ReLU</strong>: Hard cutoff at <span class="math notranslate nohighlight">\( x = 0 \)</span>, causing instability when inputs oscillate near this threshold.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU‚Äôs smooth probabilistic weighting ensures stable transitions and smoother learning dynamics.</p></li>
</ul>
</section>
</section>
<section id="problems-with-sigmoid">
<h3>2. Problems with Sigmoid<a class="headerlink" href="#problems-with-sigmoid" title="Link to this heading">#</a></h3>
<section id="a-vanishing-gradients-for-large-inputs">
<h4>a. Vanishing Gradients for Large Inputs<a class="headerlink" href="#a-vanishing-gradients-for-large-inputs" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Sigmoid</strong>: For large or small inputs, sigmoid gradients approach zero, slowing down learning.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU grows linearly for large positive inputs, avoiding saturation and keeping gradients alive. It also smoothly suppresses negative inputs without full saturation.</p></li>
</ul>
</section>
<section id="b-non-zero-mean-outputs">
<h4>b. Non-zero Mean Outputs<a class="headerlink" href="#b-non-zero-mean-outputs" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Sigmoid</strong>: Outputs between 0 and 1, causing bias in activations, slowing convergence.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU produces approximately zero-centered outputs, reducing bias and speeding up convergence.</p></li>
</ul>
</section>
</section>
<section id="problems-with-tanh">
<h3>3. Problems with Tanh<a class="headerlink" href="#problems-with-tanh" title="Link to this heading">#</a></h3>
<section id="a-vanishing-gradients-for-extreme-inputs">
<h4>a. Vanishing Gradients for Extreme Inputs<a class="headerlink" href="#a-vanishing-gradients-for-extreme-inputs" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Tanh</strong>: Saturates for large positive or negative inputs, leading to vanishing gradients.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU grows linearly for large positive inputs and suppresses large negative inputs smoothly without full saturation.</p></li>
</ul>
</section>
<section id="b-computational-complexity">
<h4>b. Computational Complexity<a class="headerlink" href="#b-computational-complexity" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Tanh</strong>: Higher computational cost compared to simpler functions like ReLU.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU approximations (e.g., using the error function or Taylor expansions) allow it to maintain competitive computational efficiency.</p></li>
</ul>
</section>
</section>
<section id="problems-with-leaky-relu">
<h3>4. Problems with Leaky ReLU<a class="headerlink" href="#problems-with-leaky-relu" title="Link to this heading">#</a></h3>
<section id="id1">
<h4>a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span><a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Leaky ReLU</strong>: Still has a non-differentiable point at <span class="math notranslate nohighlight">\( x = 0 \)</span>.</p></li>
<li><p><strong>GeLU Fix</strong>: Fully differentiable across all input values, making it smoother and more gradient-friendly.</p></li>
</ul>
</section>
<section id="b-arbitrary-slope-for-negative-inputs">
<h4>b. Arbitrary Slope for Negative Inputs<a class="headerlink" href="#b-arbitrary-slope-for-negative-inputs" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Leaky ReLU</strong>: Uses a fixed slope (usually 0.01) for negative inputs, which may not be optimal.</p></li>
<li><p><strong>GeLU Fix</strong>: Dynamically scales negative inputs based on <span class="math notranslate nohighlight">\( \Phi(x) \)</span>, adjusting the slope probabilistically.</p></li>
</ul>
</section>
</section>
<section id="complication-with-swish">
<h3>5. Complication with Swish<a class="headerlink" href="#complication-with-swish" title="Link to this heading">#</a></h3>
<section id="a-additional-learnable-parameter">
<h4>a. Additional Learnable Parameter<a class="headerlink" href="#a-additional-learnable-parameter" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Swish</strong>: Often includes a learnable parameter <span class="math notranslate nohighlight">\( \beta \)</span> to control the activation‚Äôs shape, adding complexity. Might lead to overfitting.</p></li>
</ul>
</section>
</section>
<section id="general-problems-in-activation-functions">
<h3>6. General Problems in Activation Functions<a class="headerlink" href="#general-problems-in-activation-functions" title="Link to this heading">#</a></h3>
<section id="a-saturation">
<h4>a. Saturation<a class="headerlink" href="#a-saturation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Common Activations</strong>: Many functions saturate for extreme inputs, leading to either fixed output limits (sigmoid, tanh) or nullification (ReLU for negatives).</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU avoids full saturation:</p>
<ul>
<li><p>For large positive inputs: <span class="math notranslate nohighlight">\( x \Phi(x) \approx x \)</span>, ensuring linear behavior.</p></li>
<li><p>For large negative inputs: <span class="math notranslate nohighlight">\( x \Phi(x) \approx 0 \)</span>, smoothly suppressing the output.</p></li>
</ul>
</li>
</ul>
</section>
<section id="b-lack-of-smooth-transitions">
<h4>b. Lack of Smooth Transitions<a class="headerlink" href="#b-lack-of-smooth-transitions" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Common Activations</strong>: Sharp transitions (e.g., ReLU) cause instability and optimization issues.</p></li>
<li><p><strong>GeLU Fix</strong>: GeLU provides smooth transitions everywhere, especially around <span class="math notranslate nohighlight">\( x = 0 \)</span>, where other functions struggle.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">47</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;dark_background&#39;</span><span class="p">)</span>

<span class="c1"># Define the range and standard normal CDF</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">phi_x</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))))</span>  <span class="c1"># CDF of x</span>

<span class="c1"># Compute GeLU using PyTorch&#39;s built-in function</span>
<span class="n">x_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">gelu_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>

<span class="c1"># Compute the gradient (derivative) of GeLU</span>
<span class="n">gelu_x</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_input</span><span class="p">))</span>
<span class="n">gelu_x_derivative</span> <span class="o">=</span> <span class="n">x_input</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="c1"># Compute x * Phi(x) and its derivative</span>
<span class="n">x_cdf</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">phi_x</span>
<span class="n">x_cdf</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">x_cdf_derivative</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="c1"># Plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c1"># Plot 1: CDF of Standard Normal Variate (Phi(x))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">phi_x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;CDF of Standard Normal Variate ($\Phi(x)$)&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\Phi(x)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 2: x * Phi(x)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x_cdf</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x \cdot \Phi(x)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$x \cdot \Phi(x)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 3: Derivative of x * Phi(x)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">x_cdf_derivative</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Derivative of $x \cdot \Phi(x)$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\frac</span><span class="si">{d}{dx}</span><span class="s2"> \left[ x \cdot \Phi(x) \right]$&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Plot 4: GeLU and Its Derivative (using x_input)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_input</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">gelu_x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GeLU (x)&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_input</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">gelu_x_derivative</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GeLU Derivative (x)&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;GeLU ($x$) and Its Derivative&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Adjust layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9c03b55f872dddbbbe685099f98d7c501b285448c3c65faa43cdf4e785d38ff3.png" src="../../_images/9c03b55f872dddbbbe685099f98d7c501b285448c3c65faa43cdf4e785d38ff3.png" />
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "book"
        },
        kernelOptions: {
            name: "book",
            path: "./content/generative-ai"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'book'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="attention_mechanism.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><strong>Attention Mechanism</strong></p>
      </div>
    </a>
    <a class="right-next"
       href="orthogonality.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Orthogonal Matrix</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-interactions">Key Interactions:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu-vs-common-activation-functions">GeLU vs Common Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-relu">1. Problems with ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-non-differentiability-at-x-0">a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-dead-neurons">b. Dead Neurons</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-harsh-transitions">c. Harsh Transitions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-sigmoid">2. Problems with Sigmoid</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-vanishing-gradients-for-large-inputs">a. Vanishing Gradients for Large Inputs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-non-zero-mean-outputs">b. Non-zero Mean Outputs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-tanh">3. Problems with Tanh</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-vanishing-gradients-for-extreme-inputs">a. Vanishing Gradients for Extreme Inputs</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-computational-complexity">b. Computational Complexity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problems-with-leaky-relu">4. Problems with Leaky ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">a. Non-differentiability at <span class="math notranslate nohighlight">\( x = 0 \)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-arbitrary-slope-for-negative-inputs">b. Arbitrary Slope for Negative Inputs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complication-with-swish">5. Complication with Swish</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-additional-learnable-parameter">a. Additional Learnable Parameter</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-problems-in-activation-functions">6. General Problems in Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-saturation">a. Saturation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-lack-of-smooth-transitions">b. Lack of Smooth Transitions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By @AIinMinutes
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>