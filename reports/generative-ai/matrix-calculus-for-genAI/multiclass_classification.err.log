Traceback (most recent call last):
  File "/home/aim/anaconda3/lib/python3.12/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/home/aim/anaconda3/lib/python3.12/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aim/anaconda3/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/aim/anaconda3/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/aim/anaconda3/lib/python3.12/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/home/aim/anaconda3/lib/python3.12/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/aim/anaconda3/lib/python3.12/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import random
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set aesthetic style and random seed
plt.style.use('dark_background')
SEED = 47
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)

# Functions for reusability

def cross_entropy(ps, label, num_classes):
    """Compute cross-entropy loss."""
    log_ps = torch.log(ps).reshape(num_classes, -1)
    y = torch.nn.functional.one_hot(label, num_classes=num_classes).float()
    return - y @ log_ps

def softmax_fn(logits):
    """Compute softmax."""
    return torch.softmax(logits, dim=0)

def compute_gradients(input_tensor, weight_matrix, label, num_classes):
    """Compute gradients and Jacobians for a given input, weights, and label."""
    # Forward pass
    logits = weight_matrix @ input_tensor
    logits.retain_grad()
    ps = softmax_fn(logits)
    ps.retain_grad()

    # Compute loss and gradients using AutoGrad
    loss = cross_entropy(ps, label, num_classes=num_classes)
    loss.backward(retain_graph=True)

    # Manually compute gradient using the analytical formula
    with torch.no_grad():
        ys = torch.nn.functional.one_hot(label, num_classes=num_classes).reshape(num_classes, 1)
        del_L_wrt_del_p = - ys / ps
        del_p_wrt_del_z = ps @ torch.eye(ps.shape[1]) - ps @ ps.T
        del_L_wrt_del_logits = del_p_wrt_del_z @ del_L_wrt_del_p
        del_L_wrt_W_analytical = (ps - ys) @ input_tensor.T  # Analytical gradient formula

    # Jacobians (using AutoGrad and Analytical)
    jacobian_auto_grad = torch.autograd.functional.jacobian(softmax_fn, logits)
    jacobian_analytical = torch.diag(ps.squeeze()) - ps @ ps.T

    # Results
    return {
        "grad_loss_wrt_W_correct": weight_matrix.grad,  # AutoGrad gradient
        "grad_loss_wrt_W_analytical": del_L_wrt_W_analytical,  # Analytical gradient
        "jacobian_auto_grad": jacobian_auto_grad,
        "jacobian_analytical": jacobian_analytical,
        "loss": loss.item()
    }

def plot_results(results, label, title_mode="Correct"):
    """
    Visualize gradients and Jacobians.
    :param results: Computed gradients and Jacobians.
    :param label: The label to display.
    :param title_mode: 'Correct' or 'Incorrect' to toggle the title.
    """
    title_prefix = "Correct" if title_mode == "Correct" else "Incorrect"
    fig, axes = plt.subplots(2, 2, figsize=(16, 16), dpi=300)

    # Heatmap of gradient w.r.t W (AutoGrad) in scientific notation
    sns.heatmap(
        results["grad_loss_wrt_W_correct"].detach().numpy(),
        annot=True, fmt='.2e', cmap="YlGnBu", cbar=False, ax=axes[0, 0], annot_kws={"size": 20}
    )
    axes[0, 0].set_title('Gradient w.r.t Weight Matrix (AutoGrad)', fontsize=22, weight='bold')
    axes[0, 0].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)

    # Heatmap of gradient w.r.t W (Analytical formula) in scientific notation
    sns.heatmap(
        results["grad_loss_wrt_W_analytical"].detach().numpy(),
        annot=True, fmt='.2e', cmap="coolwarm", cbar=False, ax=axes[0, 1], annot_kws={"size": 20}
    )
    axes[0, 1].set_title('Gradient w.r.t Weight Matrix (Analytical)', fontsize=22, weight='bold')
    axes[0, 1].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)

    # Heatmap of Jacobian (AutoGrad) in scientific notation
    sns.heatmap(
        results["jacobian_auto_grad"].detach().numpy().reshape(3, 3),
        annot=True, fmt='.2e', cmap="YlGnBu", cbar=False, ax=axes[1, 0], annot_kws={"size": 20}
    )
    axes[1, 0].set_title('Jacobian w.r.t Logits (AutoGrad)', fontsize=22, weight='bold')
    axes[1, 0].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)

    # Heatmap of Jacobian (Analytical) in scientific notation
    sns.heatmap(
        results["jacobian_analytical"].detach().numpy().reshape(3, 3),
        annot=True, fmt='.2e', cmap="coolwarm", cbar=False, ax=axes[1, 1], annot_kws={"size": 20}
    )
    axes[1, 1].set_title('Jacobian w.r.t Logits (Analytical)', fontsize=22, weight='bold')
    axes[1, 1].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)

    # Adjust layout for heatmaps
    fig.suptitle(f'{title_prefix} Label: Gradient and Jacobian, Loss: {results['loss']:.2f}', fontsize=30)
    plt.tight_layout()    
    fig.savefig(f'{title_prefix}.png', dpi=300)
    plt.show()

# Example usage
input_tensor = torch.tensor([[4., 7.]], requires_grad=False).reshape(2, 1)
weight_matrix = torch.randn(3, 2, requires_grad=True)
correct_label = torch.tensor([2])
incorrect_label = torch.tensor([1])
num_classes = 3

# Compute gradients
results = compute_gradients(input_tensor, weight_matrix, correct_label, num_classes)
# Plot results with title mode
plot_results(results, correct_label, title_mode="Correct")  # Change to "Incorrect" for incorrect label
------------------


[0;36m  Cell [0;32mIn[1], line 102[0;36m[0m
[0;31m    fig.suptitle(f'{title_prefix} Label: Gradient and Jacobian, Loss: {results['loss']:.2f}', fontsize=30)[0m
[0m                                                                                ^[0m
[0;31mSyntaxError[0m[0;31m:[0m f-string: unmatched '['


